{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate spatialdata_Dec24_MM\n",
    "import scanpy as sc\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import squidpy as sq\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# polygons downloaded from https://wcm.app.box.com/s/z3uev1i43xbjc88rnrz31dta5xw9jxjv\n",
    "# polygons at /Saha/Polygon_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define functions \n",
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "### Convert polygons to .json format\n",
    "def convert_polygons_to_json(\n",
    "    polygons_df,\n",
    "    x_col='x_global_px',\n",
    "    y_col='y_global_px',\n",
    "    use_subset=True,\n",
    "    subset_size=100,\n",
    "    output_file=None,\n",
    "    round_coords=False,\n",
    "    centroid_scale_factor=1.0,\n",
    "    polygon_scale_factor=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts polygon data into a JSON format where each cell is an element containing\n",
    "    a list of [x, y] coordinate pairs for all vertices. Optionally scales the centroids and polygons separately.\n",
    "\n",
    "    Parameters:\n",
    "        polygons_df (pd.DataFrame): The dataframe containing polygon data with indices as cell IDs.\n",
    "        x_col (str): Column name for x-coordinates.\n",
    "        y_col (str): Column name for y-coordinates.\n",
    "        use_subset (bool): Whether to use a subset of the data for testing.\n",
    "        subset_size (int): Number of rows to process if using a subset.\n",
    "        output_file (str): Path to save the JSON file. If None, the JSON is not saved.\n",
    "        round_coords (bool): Whether to round coordinates to 2 decimal points.\n",
    "        centroid_scale_factor (float): Factor to scale centroids. Default is 1.0 (no scaling).\n",
    "        polygon_scale_factor (float): Factor to scale polygons. Default is 1.0 (no scaling).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representation of the JSON structure.\n",
    "        float: Estimated time for processing the full dataset (if using a subset).\n",
    "    \"\"\"\n",
    "    cell_polygons = {}\n",
    "\n",
    "    if use_subset:\n",
    "        data_to_process = polygons_df.iloc[:subset_size]\n",
    "        print(f\"Processing a subset of {subset_size} rows for testing...\")\n",
    "    else:\n",
    "        data_to_process = polygons_df\n",
    "        print(\"Processing the full dataset...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate the central point of all polygons\n",
    "    central_x = data_to_process[x_col].mean()\n",
    "    central_y = data_to_process[y_col].mean()\n",
    "    central_point = Point(central_x, central_y)\n",
    "\n",
    "    # Group the data by cell ID\n",
    "    grouped_data = data_to_process.groupby(data_to_process.index)\n",
    "\n",
    "    # Process and scale points for each cell\n",
    "    for cell_id, group in grouped_data:\n",
    "        vertices = []\n",
    "        centroid_x = group[x_col].mean()\n",
    "        centroid_y = group[y_col].mean()\n",
    "\n",
    "        # Scale the centroid\n",
    "        if centroid_scale_factor != 1.0:\n",
    "            vector_x = centroid_x - central_point.x\n",
    "            vector_y = centroid_y - central_point.y\n",
    "            scaled_centroid_x = central_point.x + vector_x * centroid_scale_factor\n",
    "            scaled_centroid_y = central_point.y + vector_y * centroid_scale_factor\n",
    "        else:\n",
    "            scaled_centroid_x, scaled_centroid_y = centroid_x, centroid_y\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            x = row[x_col]\n",
    "            y = row[y_col]\n",
    "\n",
    "            # Scale the polygon vertices relative to the scaled centroid\n",
    "            if polygon_scale_factor != 1.0:\n",
    "                vector_x = x - centroid_x\n",
    "                vector_y = y - centroid_y\n",
    "                new_x = scaled_centroid_x + vector_x * polygon_scale_factor\n",
    "                new_y = scaled_centroid_y + vector_y * polygon_scale_factor\n",
    "            else:\n",
    "                new_x, new_y = x, y\n",
    "\n",
    "            if round_coords:\n",
    "                new_x = round(new_x, 2)\n",
    "                new_y = round(new_y, 2)\n",
    "\n",
    "            vertices.append([new_x, new_y])\n",
    "\n",
    "        cell_polygons[cell_id] = vertices\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    estimated_time_full_dataset = None\n",
    "    if use_subset:\n",
    "        total_rows = len(polygons_df)\n",
    "        estimated_time_full_dataset = (elapsed_time / subset_size) * total_rows\n",
    "        print(f\"\\nTime taken for {subset_size} rows: {elapsed_time:.4f} seconds\")\n",
    "        print(f\"Estimated time for full dataset ({total_rows} rows): {estimated_time_full_dataset:.2f} seconds\")\n",
    "    else:\n",
    "        print(f\"\\nTime taken for full dataset: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "    print(\"\\nFirst few entries of cell_polygons:\")\n",
    "    for key, value in islice(cell_polygons.items(), 5):\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(cell_polygons, f, indent=2)\n",
    "        print(f\"\\nJSON saved to {output_file}\")\n",
    "\n",
    "    return cell_polygons, estimated_time_full_dataset\n",
    "\n",
    "# Usage \n",
    "# full_json, _ = convert_polygons_to_json(\n",
    "#     filtered_polygons,\n",
    "#     x_col='CenterX_global_px',  # Replace with your desired x-coordinate column name\n",
    "#     y_col='CenterY_global_px',  # Replace with your desired y-coordinate column name\n",
    "#     use_subset=False,\n",
    "#     output_file='obsSegmentations_polygons.json',\n",
    "#     round_coords=True,\n",
    "#     centroid_scale_factor=1,\n",
    "#     polygon_scale_factor=1   \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ADDING MOST VARIABLE GENES ###\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "\n",
    "def extract_individual_components(adata, components, select_genes=None):\n",
    "    \"\"\"\n",
    "    Extract specified data from an AnnData object and return a DataFrame.\n",
    "    Also extracts a gene expression matrix for select_genes (user-supplied or top 30 variable genes).\n",
    "    Stores the gene expression matrix in adata.uns['genes'].\n",
    "\n",
    "    Parameters:\n",
    "    adata (AnnData): The AnnData object containing the data.\n",
    "    components (dict): A dictionary specifying the data to extract.\n",
    "    select_genes (list, optional): List of gene names to extract as a gene expression matrix.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the extracted data.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "\n",
    "    for col_name, (location, key, *index) in components.items():\n",
    "        if location == \"obsm\":\n",
    "            extracted_data[col_name] = adata.obsm[key][:, index[0]] if index else adata.obsm[key]\n",
    "        elif location == \"obs\":\n",
    "            extracted_data[col_name] = adata.obs[key]\n",
    "        elif location == \"uns\":\n",
    "            extracted_data[col_name] = adata.uns[key]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported location '{location}'. Use 'obsm', 'obs', or 'uns'.\")\n",
    "\n",
    "    df = pd.DataFrame(extracted_data, index=adata.obs_names)\n",
    "\n",
    "    # If select_genes is not provided, compute top 30 most variable genes\n",
    "    if select_genes is None:\n",
    "        sc.pp.highly_variable_genes(adata, n_top_genes=30)\n",
    "        select_genes = adata.var[adata.var['highly_variable']].index.tolist()\n",
    "        print(\"Top 30 variable genes:\", select_genes)\n",
    "    else:\n",
    "        print(\"Using user-supplied select_genes:\", select_genes)\n",
    "\n",
    "    # Check if all selected genes are present in adata.var_names\n",
    "    missing = [g for g in select_genes if g not in adata.var_names]\n",
    "    if missing:\n",
    "        print(\"Warning: These genes are missing from adata:\", missing)\n",
    "    else:\n",
    "        print(\"All selected genes are present in adata.\")\n",
    "\n",
    "    # Extract expression matrix for selected genes\n",
    "    if select_genes:  # Only proceed if the list is not empty\n",
    "        try:\n",
    "            if scipy.sparse.issparse(adata.X):\n",
    "                exp_mtx_genes = pd.DataFrame(\n",
    "                    adata[:, select_genes].X.toarray(),\n",
    "                    columns=select_genes,\n",
    "                    index=adata.obs_names\n",
    "                )\n",
    "            else:\n",
    "                exp_mtx_genes = pd.DataFrame(\n",
    "                    adata[:, select_genes].X,\n",
    "                    columns=select_genes,\n",
    "                    index=adata.obs_names\n",
    "                )\n",
    "            adata.uns['genes'] = exp_mtx_genes\n",
    "            print(f\"Shape of genes matrix: {exp_mtx_genes.shape}\")\n",
    "            print(f\"First few rows of genes matrix:\\n{exp_mtx_genes.head()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting gene expression matrix: {e}\")\n",
    "    else:\n",
    "        print(\"No genes found to extract.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def export_individual_components(df, export_config, output_path, adata=None):\n",
    "    \"\"\"\n",
    "    Export selected columns from the DataFrame or gene expression matrix to separate CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing all the data.\n",
    "    export_config (dict): A dictionary where keys are filenames and values are lists of column names to export,\n",
    "                          or the string 'genes' to export the gene expression matrix.\n",
    "    output_path (str): The directory where CSV files will be saved.\n",
    "    adata (AnnData): The AnnData object containing the gene expression matrix in adata.uns['genes'].\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename, columns in export_config.items():\n",
    "        file_path = os.path.join(output_path, filename)\n",
    "        if isinstance(columns, list):\n",
    "            missing_columns = [col for col in columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Columns {missing_columns} are not found in the DataFrame.\")\n",
    "            selected_df = df[columns]\n",
    "            selected_df.to_csv(file_path, index=False)\n",
    "            print(f\"Exported {columns} to {file_path}\")\n",
    "        elif columns == 'genes':\n",
    "            if adata is not None and 'genes' in adata.uns:\n",
    "                exp_mtx_genes = adata.uns['genes']\n",
    "                # Try to include cell_id if present in df\n",
    "                if 'cell_id' in df.columns:\n",
    "                    exp_mtx_genes_with_id = pd.concat([df[['cell_id']], exp_mtx_genes], axis=1)\n",
    "                else:\n",
    "                    exp_mtx_genes_with_id = exp_mtx_genes\n",
    "                exp_mtx_genes_with_id.to_csv(file_path, index=False)\n",
    "                print(f\"Exported gene expression matrix to {file_path} with shape {exp_mtx_genes_with_id.shape}\")\n",
    "            else:\n",
    "                raise ValueError(\"Gene expression matrix (adata.uns['genes']) is not available.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid column specification for {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Create .json Vitesse config \n",
    "def generate_json_file(version, name, description, dataset_uid, dataset_name, files, coordination_space, layout, init_strategy, output_file, url_prefix):\n",
    "    # Update the URLs in the files list with the provided prefix\n",
    "    for file in files:\n",
    "        file['url'] = f\"{url_prefix}/{file['url'].split('/')[-1]}\"\n",
    "\n",
    "    data = {\n",
    "        \"version\": version,\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"datasets\": [\n",
    "            {\n",
    "                \"uid\": dataset_uid,\n",
    "                \"name\": dataset_name,\n",
    "                \"files\": files\n",
    "            }\n",
    "        ],\n",
    "        \"coordinationSpace\": coordination_space,\n",
    "        \"layout\": layout,\n",
    "        \"initStrategy\": init_strategy\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"JSON file '{output_file}' generated successfully.\")\n",
    "\n",
    "# Example usage with the provided files\n",
    "files = [\n",
    "    {\n",
    "        \"fileType\": \"obsEmbedding.csv\",\n",
    "        \"url\": \"obsEmbedding_pca.csv\",\n",
    "        \"coordinationValues\": {\n",
    "            \"obsType\": \"cell\",\n",
    "            \"embeddingType\": \"PCA\"\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"obsIndex\": \"cell_id\",\n",
    "            \"obsEmbedding\": [\"PC_1\", \"PC_2\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"fileType\": \"obsEmbedding.csv\",\n",
    "        \"url\": \"obsEmbedding_umap.csv\",\n",
    "        \"coordinationValues\": {\n",
    "            \"obsType\": \"cell\",\n",
    "            \"embeddingType\": \"UMAP\"\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"obsIndex\": \"cell_id\",\n",
    "            \"obsEmbedding\": [\"UMAP_1\", \"UMAP_2\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"fileType\": \"obsSets.csv\",\n",
    "        \"url\": \"obsSets_cell_anno.csv\",\n",
    "        \"coordinationValues\": {\n",
    "            \"obsType\": \"cell\"\n",
    "        },\n",
    "        \"options\": {\n",
    "            \"obsIndex\": \"cell_id\",\n",
    "            \"obsSets\": [\n",
    "                {\n",
    "                    \"name\": \"Cell Anno\",\n",
    "                    \"column\": \"cell_type\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"fileType\": \"obsFeatureMatrix.csv\",\n",
    "        \"url\": \"exp_mtx_genes.csv\",\n",
    "        \"coordinationValues\": {\n",
    "            \"obsType\": \"cell\",\n",
    "            \"featureType\": \"gene\",\n",
    "            \"featureValueType\": \"expression\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"fileType\": \"obsSegmentations.json\",\n",
    "        \"url\": \"obsSegmentations_polygons.json\",\n",
    "        \"coordinationValues\": {\n",
    "            \"obsType\": \"cell\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "coordination_space = {\n",
    "    \"dataset\": {\"A\": \"D1\"},\n",
    "    \"embeddingType\": {\"A\": \"UMAP\", \"B\": \"PCA\"},\n",
    "    \"embeddingObsSetPolygonsVisible\": {\"A\": False},\n",
    "    \"embeddingObsSetLabelsVisible\": {\"A\": True},\n",
    "    \"embeddingObsSetLabelSize\": {\"A\": 16},\n",
    "    \"embeddingObsRadiusMode\": {\"A\": \"manual\"},\n",
    "    \"embeddingObsRadius\": {\"A\": 3},\n",
    "    \"embeddingZoom\": {\"TSNE\": 3, \"UMAP\": 3},\n",
    "    \"spatialZoom\": {\"A\": None},\n",
    "    \"spatialTargetX\": {\"A\": None},\n",
    "    \"spatialTargetY\": {\"A\": None},\n",
    "    \"spatialSegmentationLayer\": {\n",
    "        \"A\": {\n",
    "            \"opacity\": 1,\n",
    "            \"radius\": 0,\n",
    "            \"visible\": True,\n",
    "            \"stroked\": False\n",
    "        }\n",
    "    },\n",
    "    \"obsColorEncoding\": {\"A\": \"cellSetSelection\", \"B\": \"geneSelection\"}\n",
    "}\n",
    "\n",
    "layout = [\n",
    "    {\n",
    "        \"component\": \"scatterplot\",\n",
    "        \"coordinationScopes\": {\"dataset\": \"A\", \"embeddingType\": \"A\"},\n",
    "        \"x\": 0,\n",
    "        \"y\": 0,\n",
    "        \"w\": 4,\n",
    "        \"h\": 8\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"scatterplot\",\n",
    "        \"coordinationScopes\": {\"dataset\": \"A\", \"embeddingType\": \"B\"},\n",
    "        \"x\": 0,\n",
    "        \"y\": 4,\n",
    "        \"w\": 4,\n",
    "        \"h\": 8\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"spatial\",\n",
    "        \"props\": {\"cellRadius\": 0.34},\n",
    "        \"coordinationScopes\": {\n",
    "            \"spatialZoom\": \"A\",\n",
    "            \"spatialTargetX\": \"A\",\n",
    "            \"spatialTargetY\": \"A\",\n",
    "            \"spatialSegmentationLayer\": \"A\"\n",
    "        },\n",
    "        \"x\": 4,\n",
    "        \"y\": 0,\n",
    "        \"w\": 4,\n",
    "        \"h\": 16\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"obsSets\",\n",
    "        \"coordinationScopes\": {\"dataset\": \"A\"},\n",
    "        \"x\": 0,\n",
    "        \"y\": 8,\n",
    "        \"w\": 4,\n",
    "        \"h\": 8\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"featureList\",\n",
    "        \"x\": 4,\n",
    "        \"y\": 8,\n",
    "        \"w\": 4,\n",
    "        \"h\": 8\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"heatmap\",\n",
    "        \"props\": {\"transpose\": True},\n",
    "        \"x\": 8,\n",
    "        \"y\": 8,\n",
    "        \"w\": 4,\n",
    "        \"h\": 18\n",
    "    }\n",
    "]\n",
    "\n",
    "# ## Usage\n",
    "# generate_json_file(\n",
    "#     version=\"1.0.17\",\n",
    "#     name=\"Spatial Transcriptomics Visualization\",\n",
    "#     description=\"SAHA\",\n",
    "#     dataset_uid=\"D1\",\n",
    "#     dataset_name=\"Spatial Transcriptomics Dataset\",\n",
    "#     files=files,\n",
    "#     coordination_space=coordination_space,\n",
    "#     layout=layout,\n",
    "#     init_strategy=\"auto\",\n",
    "#     url_prefix = f(\"https://mmaycon.github.io/Spatial_Data_Visualization/Datasets/SAHA/{slide}\"),\n",
    "#     output_file=\"VitConfig_fgithub.json\"\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it for multiple slides (PROTEIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file paths (adjust if needed)\n",
    "adata_path = \"/mnt/scratch2/Luke/SAHAMaxFuse/SAHA_All_PRT_share.h5ad\" # protein data\n",
    "polygon_dir = \"/Saha/Polygon_files/\"\n",
    "\n",
    "# # Load AnnData once outside the loop\n",
    "# if 'saha_adata' not in globals():\n",
    "#     print(\"Loading AnnData...\")\n",
    "#     saha_adata = ad.read_h5ad(adata_path)\n",
    "# else:\n",
    "#     print(\"saha_adata already loaded, skipping read.\")\n",
    "\n",
    "saha_adata = ad.read_h5ad(adata_path)\n",
    "\n",
    "# Your slides list\n",
    "slides = saha_adata.obs['SAHA_name'].unique().tolist()[16]  # Add your slide names here\n",
    "if not isinstance(slides, list):\n",
    "    slides = [slides]\n",
    "\n",
    "for slide in slides:\n",
    "    print(f\"\\nProcessing slide: {slide}\")\n",
    "\n",
    "    # Slide-dependent paths\n",
    "    polygon_file_pattern = f\"{slide}-polygons.csv.gz\"\n",
    "    polygon_path = os.path.join(polygon_dir, polygon_file_pattern)\n",
    "    output_dir = os.path.join(f\"/Saha/Vitessce_view/Round_1/Objects/Datasets/SAHA/PROTEIN/{slide}_vit\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Filter AnnData by slide\n",
    "    filtered_adata = saha_adata[saha_adata.obs['SAHA_name'] == slide, :]\n",
    "\n",
    "    # 3. Load and filter polygons\n",
    "    if os.path.exists(polygon_path):\n",
    "        polygons = pd.read_csv(polygon_path)\n",
    "        cell = filtered_adata.obs['cell_id'].unique()\n",
    "        filtered_polygons = polygons[polygons['cell'].isin(cell)]\n",
    "        filtered_polygons = filtered_polygons.set_index('cell', drop=False)\n",
    "    else:\n",
    "        print(f\"Polygon file not found: {polygon_path}\")\n",
    "        filtered_polygons = pd.DataFrame()\n",
    "        # Write a failure .txt file in the output directory\n",
    "        fail_txt_path = os.path.join(output_dir, f\"failed_{slide}.txt\")\n",
    "        with open(fail_txt_path, \"w\") as fail_file:\n",
    "            fail_file.write(f\"Polygon file not found for slide: {slide}\\nPath: {polygon_path}\\n\")\n",
    "        # Skip to the next slide\n",
    "        continue\n",
    "\n",
    "    print(filtered_polygons)\n",
    "           \n",
    "    # 4. Subset AnnData to only cells with matching polygons\n",
    "    if not filtered_polygons.empty:\n",
    "        matching_cell_ids = filtered_polygons['cell'].unique()\n",
    "        filtered_adata = filtered_adata[filtered_adata.obs['cell_id'].isin(matching_cell_ids), :]\n",
    "    else:\n",
    "        print(\"No polygons to filter.\")\n",
    "\n",
    "    # 5. Visualization (if polygon coordinates exist)\n",
    "    if not filtered_polygons.empty and {'x_global_px', 'y_global_px'}.issubset(filtered_polygons.columns):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(filtered_polygons['x_global_px'], filtered_polygons['y_global_px'], c='blue', alpha=0.5, label='Cells')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title(f'Filtered Cell Polygons: {slide}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No polygon coordinates to plot.\")\n",
    "\n",
    "    # 6. Convert polygons to .json format\n",
    "    output_json_path = os.path.join(output_dir, \"obsSegmentations_polygons.json\")\n",
    "    full_json, _ = convert_polygons_to_json(\n",
    "        filtered_polygons,\n",
    "        x_col='x_global_px',\n",
    "        y_col='y_global_px',\n",
    "        use_subset=False,\n",
    "        output_file=output_json_path,\n",
    "        round_coords=False,\n",
    "        centroid_scale_factor=1,\n",
    "        polygon_scale_factor=1\n",
    "    )\n",
    "\n",
    "    # 7. Export other View types components .csv off adata \n",
    "    components = {\n",
    "        'UMAP_1': ('obsm', 'X_umap_after_harmony', 0),\n",
    "        'UMAP_2': ('obsm', 'X_umap_after_harmony', 1),\n",
    "        'PC_1': ('obsm', 'X_pca', 0),\n",
    "        'PC_2': ('obsm', 'X_pca', 1),\n",
    "        # 'leiden_cluster': ('obs', 'leiden'),\n",
    "        'cell_type': ('obs', 'celltype_mf'),\n",
    "        'cell_id': ('obs', 'cell')\n",
    "    }\n",
    "    df = extract_individual_components(filtered_adata, components, select_genes=list(filtered_adata.var_names))\n",
    "    print(df.head())\n",
    "\n",
    "    export_config = {\n",
    "        'obsEmbedding_umap.csv': ['cell_id', 'UMAP_1', 'UMAP_2'],\n",
    "        'obsEmbedding_pca.csv': ['cell_id', 'PC_1', 'PC_2'],\n",
    "        'obsSets_cell_anno.csv': ['cell_id', 'cell_type'],\n",
    "        'exp_mtx_genes.csv': 'genes'\n",
    "    }\n",
    "    export_individual_components(df, export_config, output_path=output_dir, adata=filtered_adata)\n",
    "\n",
    "    # 8. Generate the JSON file\n",
    "    generate_json_file(\n",
    "        version=\"1.0.17\",\n",
    "        name=\"Spatial Transcriptomics Visualization\",\n",
    "        description=\"SAHA\",\n",
    "        dataset_uid=\"D1\",\n",
    "        dataset_name=\"Spatial Transcriptomics Dataset\",\n",
    "        files=files,\n",
    "        coordination_space=coordination_space,\n",
    "        layout=layout,\n",
    "        init_strategy=\"auto\",\n",
    "        url_prefix = f\"https://mmaycon.github.io/Spatial_Data_Visualization/Datasets/SAHA/{slide}\",\n",
    "        output_file=f\"{output_dir}/VitConfig_fgithub.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it for multiple slides (RNA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input file paths (adjust if needed)\n",
    "# adata_path = '/mnt/scratch2/Luke/SAHAMaxFuse/SAHA_All_RNA_share.h5ad' # RNA data\n",
    "# polygon_dir = \"/Saha/Polygon_files/\"\n",
    "\n",
    "# # # Load AnnData once outside the loop\n",
    "# # if 'saha_adata' not in globals():\n",
    "# #     print(\"Loading AnnData...\")\n",
    "# #     saha_adata = ad.read_h5ad(adata_path)\n",
    "# # else:\n",
    "# #     print(\"saha_adata already loaded, skipping read.\")\n",
    "\n",
    "# saha_adata = ad.read_h5ad(adata_path)\n",
    "\n",
    "# # Your slides list\n",
    "# slides = saha_adata.obs['SAHA_name'].unique().tolist()[:]  # Add your slide names here\n",
    "\n",
    "# for slide in slides:\n",
    "#     print(f\"\\nProcessing slide: {slide}\")\n",
    "\n",
    "#     # Slide-dependent paths\n",
    "#     polygon_file_pattern = f\"{slide}-polygons.csv.gz\"\n",
    "#     polygon_path = os.path.join(polygon_dir, polygon_file_pattern)\n",
    "#     output_dir = os.path.join(f\"/Saha/Vitessce_view/Round_1/Objects/Datasets/SAHA/RNA/{slide}_vit\")\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # 2. Filter AnnData by slide\n",
    "#     filtered_adata = saha_adata[saha_adata.obs['SAHA_name'] == slide, :]\n",
    "\n",
    "#     # 3. Load and filter polygons\n",
    "#     if os.path.exists(polygon_path):\n",
    "#         polygons = pd.read_csv(polygon_path)\n",
    "#         cell = filtered_adata.obs['cell_id'].unique()\n",
    "#         filtered_polygons = polygons[polygons['cell'].isin(cell)]\n",
    "#         filtered_polygons = filtered_polygons.set_index('cell', drop=False)\n",
    "#     else:\n",
    "#         print(f\"Polygon file not found: {polygon_path}\")\n",
    "#         filtered_polygons = pd.DataFrame()\n",
    "#         # Write a failure .txt file in the output directory\n",
    "#         fail_txt_path = os.path.join(output_dir, f\"failed_{slide}.txt\")\n",
    "#         with open(fail_txt_path, \"w\") as fail_file:\n",
    "#             fail_file.write(f\"Polygon file not found for slide: {slide}\\nPath: {polygon_path}\\n\")\n",
    "#         # Skip to the next slide\n",
    "#         continue\n",
    "\n",
    "#     print(filtered_polygons)\n",
    "           \n",
    "#     # 4. Subset AnnData to only cells with matching polygons\n",
    "#     if not filtered_polygons.empty:\n",
    "#         matching_cell_ids = filtered_polygons['cell'].unique()\n",
    "#         filtered_adata = filtered_adata[filtered_adata.obs['cell_id'].isin(matching_cell_ids), :]\n",
    "#     else:\n",
    "#         print(\"No polygons to filter.\")\n",
    "\n",
    "#     # 5. Visualization (if polygon coordinates exist)\n",
    "#     if not filtered_polygons.empty and {'x_global_px', 'y_global_px'}.issubset(filtered_polygons.columns):\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "#         plt.scatter(filtered_polygons['x_global_px'], filtered_polygons['y_global_px'], c='blue', alpha=0.5, label='Cells')\n",
    "#         plt.xlabel('X')\n",
    "#         plt.ylabel('Y')\n",
    "#         plt.title(f'Filtered Cell Polygons: {slide}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"No polygon coordinates to plot.\")\n",
    "\n",
    "#     # 6. Convert polygons to .json format\n",
    "#     output_json_path = os.path.join(output_dir, \"obsSegmentations_polygons.json\")\n",
    "#     full_json, _ = convert_polygons_to_json(\n",
    "#         filtered_polygons,\n",
    "#         x_col='x_global_px',\n",
    "#         y_col='y_global_px',\n",
    "#         use_subset=False,\n",
    "#         output_file=output_json_path,\n",
    "#         round_coords=False,\n",
    "#         centroid_scale_factor=1,\n",
    "#         polygon_scale_factor=1\n",
    "#     )\n",
    "\n",
    "#     # 7. Export other View types components .csv off adata \n",
    "#     components = {\n",
    "#         'UMAP_1': ('obsm', 'X_harmony', 0),\n",
    "#         'UMAP_2': ('obsm', 'X_harmony', 1),\n",
    "#         'PC_1': ('obsm', 'X_pca', 0),\n",
    "#         'PC_2': ('obsm', 'X_pca', 1),\n",
    "#         # 'leiden_cluster': ('obs', 'leiden'),\n",
    "#         'cell_type': ('obs', 'Insitutype_Broad'),\n",
    "#         'cell_id': ('obs', 'cell')\n",
    "#     }\n",
    "#     df = extract_individual_components(filtered_adata, components, select_genes=list(filtered_adata.var_names))\n",
    "#     print(df.head())\n",
    "\n",
    "#     export_config = {\n",
    "#         'obsEmbedding_umap.csv': ['cell_id', 'UMAP_1', 'UMAP_2'],\n",
    "#         'obsEmbedding_pca.csv': ['cell_id', 'PC_1', 'PC_2'],\n",
    "#         'obsSets_cell_anno.csv': ['cell_id', 'cell_type'],\n",
    "#         'exp_mtx_genes.csv': 'genes'\n",
    "#     }\n",
    "#     export_individual_components(df, export_config, output_path=output_dir, adata=filtered_adata)\n",
    "\n",
    "#     # 8. Generate the JSON file\n",
    "#     generate_json_file(\n",
    "#         version=\"1.0.17\",\n",
    "#         name=\"Spatial Transcriptomics Visualization\",\n",
    "#         description=\"SAHA\",\n",
    "#         dataset_uid=\"D1\",\n",
    "#         dataset_name=\"Spatial Transcriptomics Dataset\",\n",
    "#         files=files,\n",
    "#         coordination_space=coordination_space,\n",
    "#         layout=layout,\n",
    "#         init_strategy=\"auto\",\n",
    "#         url_prefix = f\"https://mmaycon.github.io/Spatial_Data_Visualization/Datasets/SAHA/{slide}\",\n",
    "#         output_file=f\"{output_dir}/VitConfig_fgithub.json\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cmd\n",
    "# > jupyter nbconvert --to script /Saha/Vitessce_view/Round_1/Scripts/Get_Vitessce_view_files.ipynb\n",
    "# > conda activate spatialdata_Dec24_MM \n",
    "# > python /Saha/Vitessce_view/Round_1/Scripts/Get_Vitessce_view_files.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialdata_Dec24_MM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
